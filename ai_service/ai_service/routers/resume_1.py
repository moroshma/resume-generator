# routers/resume.py
import logging
import datetime
import traceback
from fastapi import APIRouter, Body, HTTPException, status, Response, Request
from typing import List, Dict, Any
# Import the updated NeuralService (assuming filename is neural.py)
from ai_service.services.neural_1 import NeuralService
# Keep PDF imports if needed for the PDF endpoint
from ai_service.services.pdf_generator import create_resume_pdf
from pydantic import BaseModel, Field # Import BaseModel and Field for new schemas
import httpx

# Import existing schemas (adjust paths/names if needed)
from ai_service.schemas.resume_1 import (
    UserAnswers,
    UpdateRequest,
    QuestionsResponse,
    LabelValueItem
    # Removed GeneratedSkillsResponse, UpdatedSkillsResponse as we'll redefine/rename below
)

from ai_service.config import settings

log = logging.getLogger(__name__)


class ResumeDataPdfRequest(BaseModel):
    """
    Схема запроса для генерации PDF на основе структурированных данных.
    """
    resume_data: List[LabelValueItem] = Field(..., description="Полные структурированные данные резюме (список label-value) для генерации PDF.")


# --- Define necessary Response Schemas ---

class GeneratedResumeDataResponse(BaseModel):
    """Schema for the structured data generated from all answers."""
    hard_skills: List[str] = Field(..., description="List of extracted hard skills.")
    experience_summary: str = Field(..., description="AI-generated summary of user's experience.")
    technologies: List[str] = Field(..., description="List of technologies mentioned.")

class UpdatedResumeSectionResponse(BaseModel):
    """Schema for the response when a resume section is updated."""
    updated_text: str = Field(..., description="The updated text for the resume section, generated by the AI.")



async def _get_user_info(user_service_url: str, headers: Dict[str, str]) -> Dict[str, Any]:
    """Вспомогательная функция для получения данных пользователя из user_service."""
    async with httpx.AsyncClient() as client:
        try:
            log.debug(f"Requesting user info from: {user_service_url}")
            response = await client.get(user_service_url, headers=headers)
            response.raise_for_status() # Вызовет исключение для 4xx/5xx ответов
            user_data = response.json()
            log.debug(f"Successfully retrieved user info: {list(user_data.keys())}")
            return user_data
        except httpx.HTTPStatusError as exc:
            log.error(f"HTTP error occurred while requesting user info: {exc.response.status_code} - {exc.response.text}")
            # Можно вернуть более конкретную ошибку в зависимости от статуса
            if exc.response.status_code == 404:
                 raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User info not found.")
            raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=f"User service failed with status {exc.response.status_code}.")
        except httpx.RequestError as exc:
            log.error(f"Network error occurred while requesting user info: {exc}")
            raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=f"Could not connect to user service: {exc}")
        except Exception as e:
            log.error(f"An unexpected error occurred during user info retrieval: {e}", exc_info=True)
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve user info due to an internal error.")






# --- Router Setup ---

router = APIRouter(tags=["Resume Generation API"]) # Updated tag

# Create an instance of the *updated* NeuralService
neural_service = NeuralService(settings)

# --- Endpoints ---

@router.get("/api/v001/resume/basic/question", response_model=QuestionsResponse)
async def get_base_questions():
    """
    Retrieves the initial list of base questions for the resume generation process (Stage 1).
    """
    # This endpoint reads static data from settings, no change needed here.
    return {"questions": settings.BASE_QUESTIONS}


# Use the actual NeuralService to generate follow-up questions
@router.post("/api/v001/resume/question/get", response_model=QuestionsResponse)
async def get_next_questions(user_answers: UserAnswers = Body(...)):
    """
    Generates follow-up questions (Stage 2) based on the user's answers from Stage 1.
    Requires authentication (JWT - implied, handle elsewhere).
    """
    if not user_answers.answers:
         raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Answers cannot be empty when requesting follow-up questions.",
        )

    try:
        # Call the service method to generate questions using the Gemini API
        follow_up_questions = await neural_service.generate_follow_up_questions(user_answers.answers)
        # Return the generated questions in the correct format
        return {"questions": follow_up_questions}
    except Exception as e:
        # Log the error server-side for debugging.
        print(f"Error in /api/v001/resume/question/get endpoint: {e}")
        # Raise an HTTPException for server-side errors (5xx).
        # Avoid exposing detailed internal error messages to the client in production.
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            # Provide a user-friendly error message
            detail=f"Failed to generate follow-up questions. Error: {str(e)}",
            # Optionally include specific error type/details in development/debug mode
            # detail=f"Failed to generate follow-up questions. Error: {type(e).__name__}: {str(e)}"
        )


# Use the actual NeuralService to process all answers and generate resume data
@router.post("/api/v001/resume/label/generate", response_model=List[LabelValueItem])
async def generate_resume_final(user_answers: UserAnswers = Body(...)):
    """
    Processes all user answers to generate structured resume data as a list of label/value pairs.
    """
    if not user_answers.answers:
         raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Answers cannot be empty for final resume generation.",
        )

    try:
        # neural_service.process_answers returns List[Dict[str, Any]]
        # FastAPI will validate if each item in the returned list matches LabelValueItem
        structured_data = await neural_service.process_answers(user_answers.answers)
        return structured_data
    except Exception as e:
        print(f"Error in /api/v001/resume/label/generate endpoint: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate resume data. Error: {str(e)}",
        )




# Use the actual NeuralService to update/regenerate a section
@router.post("/api/v001/resume/label/regenerate", response_model=List[LabelValueItem])
async def update_resume_section(update_req: UpdateRequest = Body(...)):
    """
    Updates resume data based on current structured data and new info,
    returning the complete, updated list of label/value pairs.
    """
    # Input validation based on the corrected UpdateRequest schema
    if not update_req.current_data and not update_req.new_info:
         raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Either current_data or new_info must be provided for regeneration.",
         )
    # You might add more specific validation if needed (e.g., new_info cannot be empty if current_data is)

    try:
        # Pass the data matching the service method's signature
        # update_resume expects: current_data: List[Dict[str, Any]], new_info: str
        updated_structured_data = await neural_service.update_resume(
            # Pydantic automatically converts update_req.current_data (List[LabelValueItem])
            # to List[Dict] when passing it, which matches the expected type hint.
            current_data=update_req.current_data,
            new_info=update_req.new_info
        )
        # FastAPI validates the returned list against List[LabelValueItem]
        return updated_structured_data
    except Exception as e:
        print(f"Error in /api/v001/resume/label/regenerate endpoint: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to update resume section. Error: {str(e)}",
        )


# PDF Generation Endpoint - Assumes skills/data are generated beforehand and passed in request
@router.post("/api/v001/resume/pdf/generate")
async def generate_resume_pdf(request: Request, request_data: ResumeDataPdfRequest = Body(...)):
    """
    Генерирует PDF резюме. Пытается получить ФИО, Email, Телефон
    из user_service и добавить их к предоставленным данным. Если получить
    данные пользователя не удалось, генерирует PDF с пустой шапкой профиля.
    Требует пересылки аутентификационных данных (заголовков/cookie) в user_service.
    """
    if not request_data.resume_data:
         raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="AI-generated resume data cannot be empty for PDF generation.",
        )

    # --- 1. Инициализация данных профиля с пустыми значениями по умолчанию ---
    profile_data_items: List[Dict[str, str]] = [
        {"label": "Full Name", "value": ""},
        {"label": "Email", "value": ""},
        {"label": "Phone", "value": ""}
    ]
    log.info("Initialized profile data with empty defaults. Attempting to fetch from user_service.")

    # --- 2. Попытка получения данных пользователя ---
    try:
        # --- Подготовка заголовков и Cookies для user_service ---
        headers_to_forward = {}
        cookies_to_forward_dict = {}

        # 2.1 Пересылка заголовка Authorization
        auth_header = request.headers.get("Authorization")
        if auth_header:
            headers_to_forward["Authorization"] = auth_header
            log.debug("Forwarding Authorization header.")

        # 2.2 Пересылка НЕОБХОДИМЫХ Cookies (например, Refresh-Token)
        refresh_token_value = request.cookies.get("Refresh-Token")
        if refresh_token_value:
            cookies_to_forward_dict["Refresh-Token"] = refresh_token_value
            log.debug("Found 'Refresh-Token' cookie to forward.")
        else:
            log.warning("Incoming request did not contain 'Refresh-Token' cookie. User service request might fail if required.")

        # Добавьте другие необходимые cookies сюда, если нужно

        # Формируем заголовок 'Cookie' для httpx
        if cookies_to_forward_dict:
            cookie_header_string = "; ".join([f"{name}={value}" for name, value in cookies_to_forward_dict.items()])
            headers_to_forward["Cookie"] = cookie_header_string
            log.debug(f"Constructed Cookie header: {cookie_header_string}")
        # --- Конец подготовки заголовков ---

        user_service_target_url = settings.USER_SERVICE_URL
        log.info(f"Attempting to fetch user info from user_service at: {user_service_target_url}")

        # --- Вызов user_service ---
        user_info = await _get_user_info(user_service_target_url, headers=headers_to_forward)

        # --- 2.1 Успех: Форматирование полученных данных ---
        full_name_parts = [user_info.get("name"), user_info.get("surname")]
        full_name = " ".join(filter(None, full_name_parts))

        # ПЕРЕЗАПИСЫВАЕМ profile_data_items с реальными данными
        profile_data_items = [
            {"label": "Full Name", "value": full_name or ""},
            {"label": "Email", "value": user_info.get("email", "") or ""},
            {"label": "Phone", "value": user_info.get("phone_number", "") or ""}
        ]
        log.info(f"Successfully fetched and formatted user profile data: {profile_data_items}")

    except HTTPException as http_exc:
         # Ошибка при запросе к user_service (напр., 401, 503). Логируем, но НЕ ПРЕРЫВАЕМ генерацию PDF.
         # Используются profile_data_items с пустыми значениями, заданные по умолчанию.
         log.warning(
             f"Could not fetch user info from user_service (HTTPException: {http_exc.status_code} - {http_exc.detail}). "
             f"Proceeding with PDF generation using an empty profile header."
         )
         # Не делаем raise http_exc
    except Exception as e:
         # Любая другая неожиданная ошибка при получении данных пользователя. Логируем, но НЕ ПРЕРЫВАЕМ.
         # Используются profile_data_items с пустыми значениями, заданные по умолчанию.
         log.error(
             f"Unexpected error while trying to fetch user info: {e}. "
             f"Proceeding with PDF generation using an empty profile header.",
             exc_info=True # Добавляем traceback в лог
         )
         # Не делаем raise

    # --- Выполнение продолжается здесь НЕЗАВИСИМО от успеха получения данных пользователя ---

    # --- 3. Объединение данных (профиль + AI) ---
    try:
        # Используем profile_data_items (либо с данными пользователя, либо пустые по умолчанию)
        ai_generated_data = [item.model_dump() if hasattr(item, 'model_dump') else item.dict()
                             for item in request_data.resume_data]
        full_resume_data = profile_data_items + ai_generated_data
        log.debug(f"Combined data for PDF generation (first 3 items): {full_resume_data[:3]}")

        # --- 4. Генерация PDF ---
        log.info("Calling PDF generation service with combined data.")
        # Функция create_resume_pdf уже должна уметь обрабатывать пустые строки в profile_data_items
        pdf_bytes = create_resume_pdf(full_resume_data)

        if not isinstance(pdf_bytes, bytes):
             log.error(f"create_resume_pdf returned unexpected type: {type(pdf_bytes)}. Expected bytes.")
             # Эта ошибка критична для генерации, поэтому здесь вызываем HTTPException
             raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Internal error: PDF generation service returned invalid data type."
             )

        # --- 5. Формирование ответа ---
        filename = f"resume_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        log.info(f"Successfully generated PDF: {filename} (User info fetch status logged previously)")
        return Response(
            content=pdf_bytes,
            media_type="application/pdf",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )

    except HTTPException as http_exc:
        # Перехватываем HTTPException, которые могли возникнуть на шагах 3, 4, 5
        log.error(f"HTTP error during PDF processing/generation: {http_exc.detail}")
        raise http_exc # Пробрасываем дальше
    except Exception as e:
        # Обработка ошибок, возникших ПОСЛЕ получения данных пользователя (на шагах 3, 4, 5)
        log.error(f"Error in PDF generation or data combination step: {e}", exc_info=True)
        traceback.print_exc() # Вывод traceback в консоль/логи для отладки
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate resume PDF after processing user info. Internal error: {str(e)}",
        )
